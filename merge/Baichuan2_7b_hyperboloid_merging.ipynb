{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_MemH_cbRWet",
    "outputId": "ab29c9a5-984b-4a09-cb14-27819c244509"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geoopt in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
      "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from geoopt) (2.5.1+cu121)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from geoopt) (1.26.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from geoopt) (1.13.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.9.0->geoopt) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.0->geoopt) (3.0.2)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: mwparserfromhell in /usr/local/lib/python3.10/dist-packages (0.6.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install geoopt\n",
    "!pip install datasets\n",
    "!pip install mwparserfromhell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ODjDogulbsQ2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "euZHIATmw0Pi"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "#Enter your token here\n",
    "HF_TOKEN_NAMAN = ''\n",
    "login(HF_TOKEN_NAMAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WKFqyQ1Mw3RE",
    "outputId": "07ce669c-dbd0-4e0d-ff21-39eb030efc65"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "baichuan_path = 'baichuan-inc/Baichuan-7B'\n",
    "baichuan_model = AutoModelForCausalLM.from_pretrained(baichuan_path, trust_remote_code=True,\n",
    "                                                   output_hidden_states=True,\n",
    "                                                  )\n",
    "baichuan_model = baichuan_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "x25FC0v8xAiF"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(baichuan_path, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k6HPYANLEO75",
    "outputId": "5f76cc97-a952-4310-8be8-56bf41614b62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20220301.aa', '20220301.ab', '20220301.ace', '20220301.ady', '20220301.af', '20220301.ak', '20220301.als', '20220301.am', '20220301.an', '20220301.ang', '20220301.ar', '20220301.arc', '20220301.arz', '20220301.as', '20220301.ast', '20220301.atj', '20220301.av', '20220301.ay', '20220301.az', '20220301.azb', '20220301.ba', '20220301.bar', '20220301.bat-smg', '20220301.bcl', '20220301.be', '20220301.be-x-old', '20220301.bg', '20220301.bh', '20220301.bi', '20220301.bjn', '20220301.bm', '20220301.bn', '20220301.bo', '20220301.bpy', '20220301.br', '20220301.bs', '20220301.bug', '20220301.bxr', '20220301.ca', '20220301.cbk-zam', '20220301.cdo', '20220301.ce', '20220301.ceb', '20220301.ch', '20220301.cho', '20220301.chr', '20220301.chy', '20220301.ckb', '20220301.co', '20220301.cr', '20220301.crh', '20220301.cs', '20220301.csb', '20220301.cu', '20220301.cv', '20220301.cy', '20220301.da', '20220301.de', '20220301.din', '20220301.diq', '20220301.dsb', '20220301.dty', '20220301.dv', '20220301.dz', '20220301.ee', '20220301.el', '20220301.eml', '20220301.en', '20220301.eo', '20220301.es', '20220301.et', '20220301.eu', '20220301.ext', '20220301.fa', '20220301.ff', '20220301.fi', '20220301.fiu-vro', '20220301.fj', '20220301.fo', '20220301.fr', '20220301.frp', '20220301.frr', '20220301.fur', '20220301.fy', '20220301.ga', '20220301.gag', '20220301.gan', '20220301.gd', '20220301.gl', '20220301.glk', '20220301.gn', '20220301.gom', '20220301.gor', '20220301.got', '20220301.gu', '20220301.gv', '20220301.ha', '20220301.hak', '20220301.haw', '20220301.he', '20220301.hi', '20220301.hif', '20220301.ho', '20220301.hr', '20220301.hsb', '20220301.ht', '20220301.hu', '20220301.hy', '20220301.ia', '20220301.id', '20220301.ie', '20220301.ig', '20220301.ii', '20220301.ik', '20220301.ilo', '20220301.inh', '20220301.io', '20220301.is', '20220301.it', '20220301.iu', '20220301.ja', '20220301.jam', '20220301.jbo', '20220301.jv', '20220301.ka', '20220301.kaa', '20220301.kab', '20220301.kbd', '20220301.kbp', '20220301.kg', '20220301.ki', '20220301.kj', '20220301.kk', '20220301.kl', '20220301.km', '20220301.kn', '20220301.ko', '20220301.koi', '20220301.krc', '20220301.ks', '20220301.ksh', '20220301.ku', '20220301.kv', '20220301.kw', '20220301.ky', '20220301.la', '20220301.lad', '20220301.lb', '20220301.lbe', '20220301.lez', '20220301.lfn', '20220301.lg', '20220301.li', '20220301.lij', '20220301.lmo', '20220301.ln', '20220301.lo', '20220301.lrc', '20220301.lt', '20220301.ltg', '20220301.lv', '20220301.mai', '20220301.map-bms', '20220301.mdf', '20220301.mg', '20220301.mh', '20220301.mhr', '20220301.mi', '20220301.min', '20220301.mk', '20220301.ml', '20220301.mn', '20220301.mr', '20220301.mrj', '20220301.ms', '20220301.mt', '20220301.mus', '20220301.mwl', '20220301.my', '20220301.myv', '20220301.mzn', '20220301.na', '20220301.nah', '20220301.nap', '20220301.nds', '20220301.nds-nl', '20220301.ne', '20220301.new', '20220301.ng', '20220301.nl', '20220301.nn', '20220301.no', '20220301.nov', '20220301.nrm', '20220301.nso', '20220301.nv', '20220301.ny', '20220301.oc', '20220301.olo', '20220301.om', '20220301.or', '20220301.os', '20220301.pa', '20220301.pag', '20220301.pam', '20220301.pap', '20220301.pcd', '20220301.pdc', '20220301.pfl', '20220301.pi', '20220301.pih', '20220301.pl', '20220301.pms', '20220301.pnb', '20220301.pnt', '20220301.ps', '20220301.pt', '20220301.qu', '20220301.rm', '20220301.rmy', '20220301.rn', '20220301.ro', '20220301.roa-rup', '20220301.roa-tara', '20220301.ru', '20220301.rue', '20220301.rw', '20220301.sa', '20220301.sah', '20220301.sat', '20220301.sc', '20220301.scn', '20220301.sco', '20220301.sd', '20220301.se', '20220301.sg', '20220301.sh', '20220301.si', '20220301.simple', '20220301.sk', '20220301.sl', '20220301.sm', '20220301.sn', '20220301.so', '20220301.sq', '20220301.sr', '20220301.srn', '20220301.ss', '20220301.st', '20220301.stq', '20220301.su', '20220301.sv', '20220301.sw', '20220301.szl', '20220301.ta', '20220301.tcy', '20220301.te', '20220301.tet', '20220301.tg', '20220301.th', '20220301.ti', '20220301.tk', '20220301.tl', '20220301.tn', '20220301.to', '20220301.tpi', '20220301.tr', '20220301.ts', '20220301.tt', '20220301.tum', '20220301.tw', '20220301.ty', '20220301.tyv', '20220301.udm', '20220301.ug', '20220301.uk', '20220301.ur', '20220301.uz', '20220301.ve', '20220301.vec', '20220301.vep', '20220301.vi', '20220301.vls', '20220301.vo', '20220301.wa', '20220301.war', '20220301.wo', '20220301.wuu', '20220301.xal', '20220301.xh', '20220301.xmf', '20220301.yi', '20220301.yo', '20220301.za', '20220301.zea', '20220301.zh', '20220301.zh-classical', '20220301.zh-min-nan', '20220301.zh-yue', '20220301.zu']\n"
     ]
    }
   ],
   "source": [
    "from datasets import get_dataset_config_names\n",
    "\n",
    "configs = get_dataset_config_names(\"wikipedia\")\n",
    "print(configs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8qAPaBkJ-s7e"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikipedia\", \"20220301.simple\", split=\"train[:50]\", trust_remote_code=True)\n",
    "texts = dataset[\"text\"]\n",
    "\n",
    "dataset = load_dataset(\"wikipedia\", \"20220301.zh-classical\", split=\"train[:50]\", trust_remote_code=True)\n",
    "texts.extend(dataset[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "ig_smp-UEw6x",
    "outputId": "8a616b97-91ae-47ad-870b-577645724a67"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'April is the fourth month of the year in the Julian and Gregorian calendars, and comes between March and May. It is one of four months to have 30 days.\\n\\nApril always begins on the same day of week as July, and additionally, January in leap years. April always ends on the same day of the week as December.\\n\\nApril\\'s flowers are the Sweet Pea and Daisy. Its birthstone is the diamond. The meaning of the diamond is innocence.\\n\\nThe Month \\n\\nApril comes between March and May, making it the fourth month of the year. It also comes first in the year out of the four months that have 30 days, as June, September and November are later in the year.\\n\\nApril begins on the same day of the week as July every year and on the same day of the week as January in leap years. April ends on the same day of the week as December every year, as each other\\'s last days are exactly 35 weeks (245 days) apart.\\n\\nIn common years, April starts on the same day of the week as October of the previous year, and in leap years, May of the previous year. In common years, April finishes on the same day of the week as July of the previous year, and in leap years, February and October of the previous year. In common years immediately after other common years, April starts on the same day of the week as January of the previous year, and in leap years and years immediately after that, April finishes on the same day of the week as January of the previous year.\\n\\nIn years immediately before common years, April starts on the same day of the week as September and December of the following year, and in years immediately before leap years, June of the following year. In years immediately before common years, April finishes on the same day of the week as September of the following year, and in years immediately before leap years, March and June of the following year.\\n\\nApril is a spring month in the Northern Hemisphere and an autumn/fall month in the Southern Hemisphere. In each hemisphere, it is the seasonal equivalent of October in the other.\\n\\nIt is unclear as to where April got its name. A common theory is that it comes from the Latin word \"aperire\", meaning \"to open\", referring to flowers opening in spring. Another theory is that the name could come from Aphrodite, the Greek goddess of love. It was originally the second month in the old Roman Calendar, before the start of the new year was put to January 1.\\n\\nQuite a few festivals are held in this month. In many Southeast Asian cultures, new year is celebrated in this month (including Songkran). In Western Christianity, Easter can be celebrated on a Sunday between March 22 and April 25. In Orthodox Christianity, it can fall between April 4 and May 8. At the end of the month, Central and Northern European cultures celebrate Walpurgis Night on April 30, marking the transition from winter into summer.\\n\\nApril in poetry \\nPoets use April to mean the end of winter. For example: April showers bring May flowers.\\n\\nEvents in April\\n\\nFixed Events \\n\\n April 1 - April Fools\\' Day\\n April 1 - Islamic Republic Day (Iran)\\n April 2 - International Children\\'s Book Day\\n April 2 - Thai Heritage and Conservation Day\\n April 2 - World Autism Awareness Day\\n April 2 - Malvinas Day (Argentina)\\n April 4 - Independence Day (Senegal)\\n April 4 - International Day for Landmine Awareness and Assistance\\n April 4 - Peace Day (Angola)\\n April 5 - End of Tax Year (United Kingdom)\\n April 6 - Tartan Day (Canada and United States)\\n April 6 - Chakri Day (Thailand)\\n April 7 - Day of Maternity and Beauty (Armenia)\\n April 7 - Genocide Memorial Day (Rwanda)\\n April 7 - World Health Day\\n April 7 - Women\\'s Day (Mozambique)\\n April 8 - Buddha\\'s Birthday (Buddhism)\\n April 9 - Martyrs\\' Day (Tunisia)\\n April 9 - Day of National Unity (Georgia)\\n April 9 - Day of the Finnish language\\n April 12 - Cosmonauts\\' Day (Russia), marking the day of Yuri Gagarin\\'s space flight\\n April 13 - Songkan (Laos), local New Year celebration\\n April 13 - Cambodian New Year\\n April 13 - Thomas Jefferson\\'s Birthday (United States)\\n April 14 - Southeast Asian New Year festivals, including Songkran\\n April 14 - Georgian language Day\\n April 14 - Youth Day (Angola)\\n April 14 - Ambedkar Tayanti (India)\\n April 14 - Pan-American Day\\n April 15 - Tax Day (United States)\\n April 15 - Kim Il-Sung\\'s Birthday (North Korea)\\n April 15 - Father Damien Day (Hawaii)\\n April 15 - Jackie Robinson Day (Major League Baseball)\\n April 16 - Birthday of Queen Margrethe II of Denmark\\n April 16 - Emancipation Day (Washington, DC)\\n April 16 - World Voice Day\\n April 16 - Selena Day (Texas)\\n April 17 - National Day of Syria\\n April 17 - Flag Day (American Samoa)\\n April 17 - Women\\'s Day (Gabon)\\n April 17 - World Hemophilia Day\\n April 18 - Independence Day (Zimbabwe)\\n April 18 - Invention Day (Japan)\\n April 18 - International Day of Monuments and Sites\\n April 19 - Bicycle Day\\n April 19 - Dutch-American Friendship Day\\n April 19 - Birthday of King Mswati III of Swaziland\\n April 19 - Patriots\\' Day (Massachusetts, Maine, Wisconsin)\\n April 20 - 4/20 in Cannabis Culture\\n April 21 - John Muir Day (California)\\n April 21 - San Jacinto Day (Texas)\\n April 21 - Kartini Day (Indonesia)\\n April 21 - National Tree Planting Day (Kenya)\\n April 21 - First Day of Ridran (Baha\\'i faith)\\n April 21 - Grounation Day (Rastafari movement)\\n April 22 - Earth Day\\n April 22 - Discovery Day (Brazil)\\n April 23 - Saint George\\'s Day, celebrating the patron saint of several countries, regions and cities (including England and Catalonia)\\n April 23 - World Book Day\\n April 23 - National Sovereignty and Children\\'s Day (Turkey)\\n April 24 - Democracy Day (Nepal)\\n April 24 - Genocide Day (Armenia)\\n April 24 - Republic Day (the Gambia)\\n April 25 - Australia and New Zealand celebrate ANZAC Day. ANZAC  means Australian and New Zealand Army Corps, and began in 1915.\\n April 25 - World DNA Day\\n April 25 - World Malaria Day\\n April 25 - Flag Day (Swaziland, Faroe Islands)\\n April 25 - Freedom Day (Portugal)\\n April 25 - Liberation Day (Italy)\\n April 25 - Army Day (North Korea)\\n April 26 - Union Day (Tanzania)\\n April 26 - Confederate Memorial Day (Texas, Florida)\\n April 27 - Independence Day (Sierra Leone and Togo)\\n April 27 - Freedom Day (South Africa)\\n April 27 - World Tapir Day\\n April 27 - King\\'s Day (Netherlands) from 2014, birthday of Willem-Alexander of the Netherlands\\n April 28 - Workers Memorial Day\\n April 28 - National Day (Sardinia)\\n April 28 - National Heroes Day (Barbados)\\n April 29 - Showa Day (Japan), birthday of Emperor Hirohito, who died in 1989\\n April 29 - International Dance Day\\n April 30 - Former Queen\\'s Day Holiday in the Netherlands (changed to King\\'s Day, April 27 in 2014), was the birthday of former Queen Juliana of the Netherlands\\n April 30 - Flag Day in Sweden (birthday of King Carl XVI Gustaf of Sweden)\\n April 30 - International Jazz Day\\n April 30 - Walpurgis Night (Central and Northern Europe)\\n\\nMoveable Events \\n\\n Easter-related events in Western Christianity:\\n Palm Sunday (between March 15 and April 18)\\n Maundy Thursday (between March 19 and April 22)\\n Good Friday (between March 20 and April 23)\\n Easter Sunday (between March 22 and April 25)\\n Easter Monday (between March 23 and April 26)\\n Eastern Orthodox Easter falls between April 4 and May 8.\\n Ascension Day (Western Christianity), falls between April 30 and June 3.\\n Jewish Passover - falls in the same week as Western Christianity\\'s Holy Week, which is the week leading up to Easter.\\n Mother\\'s Day (UK) falls between March 1 and April 4.\\n World Snooker Championship (late April, early May)\\n Horse racing - Grand National (UK), Kentucky Derby (United States)\\n Start of Daylight Saving Time - Clocks going forward one hour:\\n Most of Mexico\\n Morocco (Ramadan does not include Daylight Saving Time)\\n End of Daylight Saving Time - Clocks going back one hour:\\n Southeast Australia, and New Zealand\\n Chile\\n Marathon Events in the following cities:\\n Belgrade, Serbia\\n Boston, Massachusetts, United States\\n Brighton, United Kingdom\\n Enschede, Netherlands\\n London, United Kingdom\\n Madrid, Spain\\n Paris, France\\n Rotterdam, Netherlands\\n Utrecht, Netherlands\\n Zurich, Switzerland\\n\\nSelection of Historical Events \\n\\n April 1, 1918 - The Royal Air Force is founded.\\n April 1, 1976 - Apple Inc. is founded.\\n April 1, 1979 - The Islamic Republic of Iran is founded.\\n April 1, 1999 - The territory of Nunavut is created in Northern Canada.\\n April 1, 2001 - The Netherlands introduces same-sex marriage, as the first country to do so.\\n April 2, 1519 - Florida is sighted by a European for the first time.\\n April 2, 1930 - Haile Selassie becomes Emperor of Ethiopia.\\n April 2, 1982 - Start of the Falklands War, as Argentine forces land on the Falkland Islands.\\n April 2, 2005 - Pope John Paul II dies aged 84, after 26-and-a-half years as Pope.\\n April 3, 1973 - The first-ever mobile phone call is placed by Martin Cooper in New York City.\\n April 4, 1721 - Robert Walpole becomes the first Prime Minister of Great Britain.\\n April 4, 1841 - William Henry Harrison dies. He was President of the United States for 31 days, the shortest-ever time in office for a US President.\\n April 4, 1960 - Senegal becomes independent.\\n April 4, 1968 - Assassination of Martin Luther King, Jr. in Memphis, Tennessee.\\n April 5, 1722 - Jacob Roggeveen becomes the first European to land on Easter Island, landing there on Easter Sunday.\\n April 6, 1320 - Scotland\\'s independence is confirmed with the Declaration of Arbroath.\\n April 6, 1830 - The Mormon Church is founded.\\n April 6, 1909 - Robert Peary claims to have been first at the North Pole on this date.\\n April 7, 1994 - The Rwandan Genocide begins.\\n April 9, 1865 - American Civil War: Confederate forces under Robert E. Lee surrender to Union forces.\\n April 9, 1940 - World War II: Denmark and Norway are invaded by Nazi Germany.\\n April 9, 1989 - April 9 tragedy: In Tbilisi, Georgia, a peaceful demonstration for independence is broken up by the Soviet Army, killing 20 people. The country gains independence on this date exactly two years later.\\n April 10, 1815 - Mount Tambora in Indonesia erupts in a huge eruption, affecting the world\\'s climate for at least a year.\\n April 10, 2010 - A plane crash near Smolensk, Russia, kills several people who were important in Poland, including President Lech Kaczynski.\\n April 11, 1814 - Napoleon Bonaparte is exiled to the island of Elba.\\n April 11, 1954 - Said to have been the most boring day of the 20th century.\\n April 12, 1861 - The American Civil War begins at Fort Sumter, Charleston, South Carolina.\\n April 12, 1945 - US President Franklin D. Roosevelt dies, and Harry S. Truman replaces him.\\n April 12, 1961 - Yuri Gagarin becomes the first human to fly into space.\\n April 14, 1865 - US President Abraham Lincoln is shot dead at Ford\\'s Theatre by John Wilkes Booth. Lincoln dies the next day.\\n April 14, 2010 - Qinghai Province, China, is hit by an earthquake, killing tens of thousands of people.\\n April 14, 2010 - The eruption of Eyjafjallajokull in Iceland shuts down air traffic around Europe for a week, due to its ash cloud.\\n April 15, 1912 - The ship RMS Titanic sinks near Newfoundland after hitting an iceberg, resulting in the deaths of many of the people on board.\\n April 16, 1943 - Albert Hofmann discovers LSD\\'s effects.\\n April 17, 1946 - Syria gains full independence from France.\\n April 18, 1906 - 1906 San Francisco earthquake: San Francisco, California, is hit by a big earthquake, resulting in fires that destroy large parts of the city.\\n April 18, 1980 - Zimbabwe gains full independence.\\n April 19, 1897 - The first Boston Marathon is held.\\n April 19, 1971 - Sierra Leone becomes a republic.\\n April 19, 1993 - The siege of the Branch Davidians at Waco, Texas, ends in a fire that kills 82 people.\\n April 19, 1995 - Timothy McVeigh carries out the Oklahoma City bombing, killing 169 people.\\n April 19, 2005 - Joseph Alois Ratzinger becomes Pope Benedict XVI.\\n April 20, 1902 - Marie Curie and Pierre Curie refine Radium.\\n April 20, 2010 - Deepwater Horizon oil spill: A massive fire on the Deepwater Horizon drilling rig in the Gulf of Mexico kills 11 workers and causes a massive oil spill, the worst spill in US history.\\n April 21, 753 BC - Legendary founding date of Rome\\n April 21, 1509 - Henry VIII of England becomes King.\\n April 21, 1908 - Frederick Cook claims to have reached the North Pole on this date.\\n April 22, 1502 - Pedro Alvares Cabral becomes the first European to reach present-day Brazil.\\n April 22, 1970 - Earth Day is observed for the first time.\\n April 23, 1533 - The Church of England declares that Henry VIII of England and Catherine of Aragon are not married.\\n April 24, 1916 - The Easter Rising occurs in Dublin, Ireland.\\n April 24, 1990 - The Hubble Space Telescope is launched on the Space Shuttle Discovery.\\n April 25, 1915 - World War I: In Turkey, the Battle of Gallipoli begins, Australian, French, British and New Zealand forces land at Anzac cove.\\n April 25, 1974 - Portugal\\'s dictatorship is overthrown in a coup, in what is known as the Carnation Revolution.\\n April 26, 1937 - Spanish Civil War: German planes bomb the town of Guernica, Basque Country, later depicted in a painting by Pablo Picasso.\\n April 26, 1964 - Tanganyika and Zanzibar merge to form Tanzania.\\n April 26, 1986 - A reactor explosion occurs at the Chernobyl nuclear plant in present-day Ukraine, with radiation spreading around Europe and the world.\\n April 26/27, 1994 - South Africa holds its first free elections.\\n April 27, 1960 - Togo becomes independent from France.\\n April 27, 1961 - Sierra Leone becomes independent from the United Kingdom.\\n April 28, 1789 - Mutiny on the ship Bounty in the Pacific Ocean, lead by Fletcher Christian.\\n April 28, 1945 - Benito Mussolini is executed by Italian partisans.\\n April 28, 1947 - In Peru, Thor Heyerdahl starts his Kon-Tiki expedition aimed at proving his theory that the Polynesian settlers on the Pacific Ocean\\'s islands came from South America.\\n April 29, 1991 - A cyclone in Bangladesh kills an estimated 138,000 people.\\n April 29, 2011 - The wedding of Prince William, Duke of Cambridge and Catherine, Duchess of Cambridge is broadcast worldwide.\\n April 30, 1789 - George Washington becomes the first President of the United States.\\n April 30, 1803 - The United States purchases (buys) the Louisiana territory from France.\\n April 30, 1945 - Adolf Hitler commits suicide on the same day that the Soviet Army raises the Red Flag on Berlin\\'s Reichstag.\\n April 30, 1952 - The Diary of Anne Frank is published in English.\\n April 30, 1975 - The Vietnam War ends, as North Vietnamese forces take Saigon.\\n April 30, 1980 - Queen Juliana of the Netherlands abdicates the throne, and her daughter becomes Queen Beatrix of the Netherlands. Beatrix later also abdicates, on this day in 2013, in favor of her son, King Willem-Alexander of the Netherlands.\\n\\nTrivia \\n\\n In Western Christianity, there is a bigger likelihood of Easter falling in April than in March.\\n The months around April (March and May) both start with an \\'M\\' in the English language, with an \\'A\\' as the second letter.\\n In the English language, April is the first of three months in-a-row, along with May and June, that is also a female given name.\\n The astrological signs for April are Aries (March 21 to April 20) and Taurus (April 21 to May 20).\\n The sweet pea and daisy are the traditional birth flowers for April.\\n Birthstone for April is the Diamond.\\nApril 1 is the only day in April to start within the first quarter of the calendar year.\\n If the months of the year were arranged in alphabetical order in the English language, April would come first.\\n Six current European monarchs were born in April. They are King Philippe of Belgium (April 15), Queen Margrethe II of Denmark (April 16), Henri, Grand Duke of Luxembourg (April 16), Elizabeth II of the United Kingdom and Commonwealth realms (April 21), King Willem-Alexander of the Netherlands (April 27), and King Carl XVI Gustaf of Sweden (April 30).\\n\\nReferences'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JdeIZNp5yqYP"
   },
   "outputs": [],
   "source": [
    "def process_text_in_chunks(text, tokenizer, model, chunk_size=512, max_length=512):\n",
    "    \"\"\"\n",
    "    Process a single text input in smaller chunks to avoid memory overflow.\n",
    "    Returns accumulated activations for each layer.\n",
    "    \"\"\"\n",
    "    # Tokenize text\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "    input_ids = tokens[\"input_ids\"][0]  # Get the first sequence\n",
    "\n",
    "    # Get the number of layers from the model's configuration\n",
    "    num_layers = model.config.num_hidden_layers\n",
    "\n",
    "    # Initialize accumulation list on CPU\n",
    "    accumulated_hidden_states = [torch.zeros(0, model.config.hidden_size).to(\"cpu\") for _ in range(num_layers)]\n",
    "\n",
    "    # Process input in chunks\n",
    "    for i in range(0, len(input_ids), chunk_size):\n",
    "        chunk = input_ids[i : i + chunk_size].unsqueeze(0).to(\"cuda\")  # Move to GPU\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=chunk, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states  # List of tensors, one per layer\n",
    "            #print(\"Length of hidden states\", len(hidden_states))\n",
    "        # Offload activations for each layer to CPU and accumulate\n",
    "        for layer_idx, layer_activation in enumerate(hidden_states[1:]):\n",
    "            #print(layer_idx)\n",
    "            accumulated_hidden_states[layer_idx] = torch.cat(\n",
    "                [accumulated_hidden_states[layer_idx], layer_activation.squeeze(0).to(\"cpu\")], dim=0\n",
    "            )\n",
    "\n",
    "        # Free GPU memory\n",
    "        del chunk, hidden_states\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return accumulated_hidden_states\n",
    "\n",
    "# Step 4: Process all samples efficiently\n",
    "def process_all_samples(texts, tokenizer, model, chunk_size=512):\n",
    "    \"\"\"\n",
    "    Process all texts and compute accumulated hidden states for each layer.\n",
    "    \"\"\"\n",
    "    # Get the number of layers from the model's configuration\n",
    "    num_layers = model.config.num_hidden_layers\n",
    "\n",
    "    # Initialize global accumulation list for all layers\n",
    "    global_accumulated_hidden_states = [torch.zeros(0, model.config.hidden_size).to(\"cpu\") for _ in range(num_layers)]\n",
    "\n",
    "    for text in texts:\n",
    "        sample_hidden_states = process_text_in_chunks(text, tokenizer, model, chunk_size)\n",
    "\n",
    "        # Accumulate results across all samples for each layer\n",
    "        for layer_idx, layer_activation in enumerate(sample_hidden_states):\n",
    "            global_accumulated_hidden_states[layer_idx] = torch.cat(\n",
    "                [global_accumulated_hidden_states[layer_idx], layer_activation], dim=0\n",
    "            )\n",
    "        # Free CPU memory for intermediate sample_hidden_states\n",
    "        del sample_hidden_states\n",
    "\n",
    "    return global_accumulated_hidden_states\n",
    "\n",
    "\n",
    "def compute_layer_averages(global_accumulated_hidden_states, num_samples):\n",
    "    \"\"\"\n",
    "    Compute average hidden states for each layer, collapsing both tokens and samples.\n",
    "    \"\"\"\n",
    "    return [layer_hidden_states.mean(dim=0, keepdim=True).mean(dim=0, keepdim=True) for layer_hidden_states in global_accumulated_hidden_states]\n",
    "\n",
    "# Execute the pipeline\n",
    "chunk_size = 128\n",
    "global_hidden_states = process_all_samples(texts, tokenizer, baichuan_model, chunk_size)\n",
    "averaged_hidden_states = compute_layer_averages(global_hidden_states, len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgDKZ7u-AxBI"
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('averaged_hidden_states.pkl', 'wb') as f:\n",
    "#     pickle.dump(averaged_hidden_states, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "xenAGOzlB6MC"
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('averaged_hidden_states.pkl', 'rb') as f:\n",
    "#     averaged_hidden_states = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "F3abBm3X5pUK"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "def exp_map0(x: Tensor, curv: float | Tensor = 1.0, eps: float = 1e-8) -> Tensor:\n",
    "    \"\"\"\n",
    "    Map points from the tangent space at the vertex of hyperboloid, on to the\n",
    "    hyperboloid. This mapping is done using the exponential map of Lorentz model.\n",
    "\n",
    "    Args:\n",
    "        x: Tensor of shape `(B, D)` giving batch of Euclidean vectors to project\n",
    "            onto the hyperboloid. These vectors are interpreted as velocity\n",
    "            vectors in the tangent space at the hyperboloid vertex.\n",
    "        curv: Positive scalar denoting negative hyperboloid curvature.\n",
    "        eps: Small float number to avoid division by zero.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of same shape as `x`, giving space components of the mapped\n",
    "        vectors on the hyperboloid.\n",
    "    \"\"\"\n",
    "\n",
    "    rc_xnorm = curv**0.5 * torch.norm(x, dim=-1, keepdim=True)\n",
    "\n",
    "    # Ensure numerical stability in sinh by clamping input.\n",
    "    sinh_input = torch.clamp(rc_xnorm, min=eps, max=math.asinh(2**15))\n",
    "    _output = torch.sinh(sinh_input) * x / torch.clamp(rc_xnorm, min=eps)\n",
    "    return _output\n",
    "\n",
    "def pairwise_inner(x: Tensor, y: Tensor, curv: float | Tensor = 1.0):\n",
    "    \"\"\"\n",
    "    Compute pairwise Lorentzian inner product between input vectors.\n",
    "\n",
    "    Args:\n",
    "        x: Tensor of shape `(B1, D)` giving a space components of a batch\n",
    "            of vectors on the hyperboloid.\n",
    "        y: Tensor of shape `(B2, D)` giving a space components of another\n",
    "            batch of points on the hyperboloid.\n",
    "        curv: Positive scalar denoting negative hyperboloid curvature.\n",
    "        eps: Small float number to avoid numerical instability.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape `(B1, B2)` giving pairwise Lorentzian inner product\n",
    "        between input vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    x_time = torch.sqrt(1 / curv + torch.sum(x**2, dim=-1, keepdim=True))\n",
    "    y_time = torch.sqrt(1 / curv + torch.sum(y**2, dim=-1, keepdim=True))\n",
    "    xyl = x @ y.T - x_time @ y_time.T\n",
    "    return xyl\n",
    "\n",
    "\n",
    "def pairwise_dist(\n",
    "    x: Tensor, y: Tensor, curv: float | Tensor = 1.0, eps: float = 1e-8\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Compute the pairwise geodesic distance between two batches of points on\n",
    "    the hyperboloid.\n",
    "\n",
    "    Args:\n",
    "        x: Tensor of shape `(B1, D)` giving a space components of a batch\n",
    "            of point on the hyperboloid.\n",
    "        y: Tensor of shape `(B2, D)` giving a space components of another\n",
    "            batch of points on the hyperboloid.\n",
    "        curv: Positive scalar denoting negative hyperboloid curvature.\n",
    "        eps: Small float number to avoid numerical instability.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape `(B1, B2)` giving pairwise distance along the geodesics\n",
    "        connecting the input points.\n",
    "    \"\"\"\n",
    "\n",
    "    c_xyl = -curv * pairwise_inner(x, y, curv)\n",
    "    _distance = torch.acosh(torch.clamp(c_xyl, min=1 + eps))\n",
    "    return _distance / curv**0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xYwjEFbUGuoC",
    "outputId": "845bccd2-e978-437d-e22b-e31d7824dbfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proj_embeddings:\n",
      "tensor([[-1.4257e+00, -2.7491e+00, -1.4808e+00,  ..., -1.7245e+00,\n",
      "          3.9758e+00,  1.1430e+00],\n",
      "        [-1.7000e+01, -1.2098e+01,  1.5052e+01,  ...,  7.9979e+00,\n",
      "          6.0188e+01,  6.1079e+01],\n",
      "        [-3.6613e+01, -4.5963e+01,  5.0800e+01,  ..., -8.6555e+00,\n",
      "          9.7909e+01,  1.1145e+02],\n",
      "        ...,\n",
      "        [ 2.7265e+02, -1.8691e+02, -1.4212e+01,  ...,  8.9775e+01,\n",
      "         -1.3123e+02,  3.0310e+00],\n",
      "        [ 6.6495e+02, -4.8696e+02, -5.5979e+02,  ...,  4.1126e+02,\n",
      "         -5.1421e+02,  7.0104e+02],\n",
      "        [ 5.1030e+02, -1.2849e+02,  2.0052e+02,  ...,  7.1778e+02,\n",
      "         -5.0017e+02,  1.3480e+03]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Step 1: Project each layer's hidden state onto the hyperboloid\n",
    "def project_layers_to_hyperboloid(averaged_hidden_states, curv=1.0):\n",
    "    \"\"\"\n",
    "    Projects each layer's averaged hidden state onto the Lorentz hyperboloid.\n",
    "    Args:\n",
    "        averaged_hidden_states (list of torch.Tensor): List of tensors, each of shape [1, hidden_size].\n",
    "        curv (float): Curvature of the hyperboloid.\n",
    "    Returns:\n",
    "        torch.Tensor: Projected embeddings on the hyperboloid of shape [num_layers, hidden_size].\n",
    "    \"\"\"\n",
    "    # Apply exp_map0 to each tensor in the list\n",
    "    projected_embeddings = torch.cat([exp_map0(state, curv=curv) for state in averaged_hidden_states], dim=0)\n",
    "    return projected_embeddings\n",
    "\n",
    "proj_embeddings = project_layers_to_hyperboloid(averaged_hidden_states, curv=1.0)\n",
    "\n",
    "print(\"proj_embeddings:\")\n",
    "print(proj_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-KPbwn43Bod",
    "outputId": "62e7343b-f54e-47f0-fbf9-c6f3075a4b67"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4096])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_YGDoxYqr1eg",
    "outputId": "4c9033b6-19e9-4fce-b000-85202715bf5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters sorted by maximum similarity:\n",
      "Cluster 4: Layers [20, 21, 22], Max Similarity = 211.8329\n",
      "Cluster 0: Layers [23, 24, 25, 26, 27, 28, 29], Max Similarity = 83.4615\n",
      "Cluster 3: Layers [0, 8, 9, 10, 11, 12, 13, 14, 15, 16], Max Similarity = 14.7306\n",
      "Cluster 2: Layers [1, 2, 3, 4, 5, 6, 7], Max Similarity = -23.4180\n",
      "Cluster 5: Layers [17, 18, 19], Max Similarity = -141.4587\n",
      "Cluster 1: Layers [30, 31], Max Similarity = -539.7925\n",
      "\n",
      "Top Clusters:\n",
      "Cluster 4: Layers [20, 21, 22], Average Similarity = 211.8329\n",
      "Cluster 0: Layers [23, 24, 25, 26, 27, 28, 29], Average Similarity = 83.4615\n",
      "Cluster 3: Layers [0, 8, 9, 10, 11, 12, 13, 14, 15, 16], Average Similarity = 14.7306\n",
      "Cluster 2: Layers [1, 2, 3, 4, 5, 6, 7], Average Similarity = -23.4180\n",
      "Cluster 5: Layers [17, 18, 19], Average Similarity = -141.4587\n",
      "Cluster 1: Layers [30, 31], Average Similarity = -539.7925\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def hyperbolic_spectral_clustering(projected_embeddings, n_clusters=5):\n",
    "    \"\"\"\n",
    "    Perform spectral clustering using hyperbolic inner product.\n",
    "    \"\"\"\n",
    "    def hyperbolic_inner_product(x, y, curv=1.0):\n",
    "        x_time = torch.sqrt(1 / curv + torch.sum(x**2, dim=-1, keepdim=True))\n",
    "        y_time = torch.sqrt(1 / curv + torch.sum(y**2, dim=-1, keepdim=True))\n",
    "        return x @ y.T - x_time @ y_time.T\n",
    "\n",
    "    flat_embeddings = projected_embeddings.squeeze(1)\n",
    "\n",
    "    affinity_matrix = hyperbolic_inner_product(flat_embeddings, flat_embeddings).cpu().numpy()\n",
    "\n",
    "    affinity_matrix = (affinity_matrix - affinity_matrix.min()) / (affinity_matrix.max() - affinity_matrix.min())\n",
    "\n",
    "    from sklearn.cluster import SpectralClustering\n",
    "    clustering = SpectralClustering(\n",
    "        n_clusters=n_clusters,\n",
    "        affinity='precomputed',\n",
    "        random_state=21\n",
    "    ).fit(affinity_matrix)\n",
    "\n",
    "    cluster_labels = clustering.labels_\n",
    "\n",
    "    clusters = {}\n",
    "    for cluster in range(n_clusters):\n",
    "        cluster_indices = np.where(cluster_labels == cluster)[0]\n",
    "        clusters[cluster] = {\n",
    "            'layers': cluster_indices.tolist()\n",
    "        }\n",
    "\n",
    "    return clusters, cluster_labels\n",
    "\n",
    "def compute_cluster_similarities(clusters, similarity_matrix):\n",
    "    \"\"\"\n",
    "    Compute intra-cluster similarities and identify top clusters.\n",
    "    \"\"\"\n",
    "    # Convert similarity matrix to numpy if it's a torch tensor\n",
    "    if torch.is_tensor(similarity_matrix):\n",
    "        similarity_matrix = similarity_matrix.numpy()\n",
    "\n",
    "    # Store cluster similarities\n",
    "    cluster_similarities = {}\n",
    "\n",
    "    # Compute similarities for each cluster\n",
    "    for label, cluster_info in clusters.items():\n",
    "        cluster_indices = cluster_info['layers']\n",
    "\n",
    "        # Extract intra-cluster similarity matrix\n",
    "        cluster_similarities_matrix = similarity_matrix[cluster_indices][:, cluster_indices]\n",
    "\n",
    "        # Compute average similarity within the cluster\n",
    "        avg_similarity = np.mean(cluster_similarities_matrix)\n",
    "        cluster_similarities[label] = avg_similarity\n",
    "\n",
    "    # Sort clusters by average similarity\n",
    "    sorted_clusters = sorted(cluster_similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get top clusters\n",
    "    top_clusters = {label: cluster_similarities[label] for label, _ in sorted_clusters}\n",
    "\n",
    "    return clusters, cluster_similarities, top_clusters\n",
    "\n",
    "# Usage\n",
    "clusters, labels = hyperbolic_spectral_clustering(proj_embeddings, 6)\n",
    "clusters_dict, similarities, top_clusters = compute_cluster_similarities(clusters, proj_embeddings)\n",
    "\n",
    "# Sort clusters by their maximum similarity in descending order\n",
    "sorted_clusters = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print results\n",
    "print(\"Clusters sorted by maximum similarity:\")\n",
    "for label, sim in sorted_clusters:\n",
    "    layers = clusters_dict[label]['layers']  # Get layers associated with the cluster\n",
    "    print(f\"Cluster {label}: Layers {layers}, Max Similarity = {sim:.4f}\")\n",
    "\n",
    "print(\"\\nTop Clusters:\")\n",
    "for label, sim in top_clusters.items():\n",
    "    layers = clusters_dict[label]['layers']\n",
    "    print(f\"Cluster {label}: Layers {layers}, Average Similarity = {sim:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "joQftGIQgq1T"
   },
   "outputs": [],
   "source": [
    "def merge_layers_in_place_baichuan(model, merge_base_lay, merge_layer_num):\n",
    "    # Determine the number of layers to merge, ensuring we don't exceed model limits\n",
    "    merge_layer_num = min(merge_layer_num, len(model.model.layers) - merge_base_lay - 1)\n",
    "\n",
    "    for diff_lay in range(merge_base_lay + 1, merge_base_lay + 1 + merge_layer_num):\n",
    "        print(f\"Merging layer {diff_lay} into layer {merge_base_lay}\")\n",
    "\n",
    "        # Retrieve the base and merging layers\n",
    "        base_layer = model.model.layers[merge_base_lay]\n",
    "        merging_layer = model.model.layers[diff_lay]\n",
    "\n",
    "        # Update MLP weights: gate_proj, down_proj, up_proj\n",
    "        base_layer.mlp.gate_proj.weight.data.add_(\n",
    "            merging_layer.mlp.gate_proj.weight.data - base_layer.mlp.gate_proj.weight.data\n",
    "        )\n",
    "        base_layer.mlp.down_proj.weight.data.add_(\n",
    "            merging_layer.mlp.down_proj.weight.data - base_layer.mlp.down_proj.weight.data\n",
    "        )\n",
    "        base_layer.mlp.up_proj.weight.data.add_(\n",
    "            merging_layer.mlp.up_proj.weight.data - base_layer.mlp.up_proj.weight.data\n",
    "        )\n",
    "\n",
    "        # Update self-attention weights: W_pack, o_proj\n",
    "        base_layer.self_attn.W_pack.weight.data.add_(\n",
    "            merging_layer.self_attn.W_pack.weight.data - base_layer.self_attn.W_pack.weight.data\n",
    "        )\n",
    "        base_layer.self_attn.o_proj.weight.data.add_(\n",
    "            merging_layer.self_attn.o_proj.weight.data - base_layer.self_attn.o_proj.weight.data\n",
    "        )\n",
    "\n",
    "    # Remove merged layers in reverse order to avoid shifting indices\n",
    "    layers_to_delete = list(range(merge_base_lay + merge_layer_num, merge_base_lay, -1))\n",
    "    for diff_lay in layers_to_delete:\n",
    "        print(f\"Deleting layer {diff_lay}\")\n",
    "        del model.model.layers[diff_lay]\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "W3evACd_KqXf"
   },
   "outputs": [],
   "source": [
    "def merge_clusters(model, clusters, cluster_similarities, threshold, merge_layers_fn):\n",
    "    \"\"\"\n",
    "    Merge model layers based on clusters and their average similarities.\n",
    "\n",
    "    Args:\n",
    "    - model: The model to merge layers in.\n",
    "    - clusters: Dictionary of clusters with layers as lists.\n",
    "    - cluster_similarities: Dictionary with cluster IDs as keys and average similarities as values.\n",
    "    - threshold: Similarity threshold to decide whether to merge a cluster.\n",
    "    - merge_layers_fn: Function to handle merging of layers (e.g., merge_layers_return_model).\n",
    "\n",
    "    Returns:\n",
    "    - model: The updated model with merged layers.\n",
    "    \"\"\"\n",
    "    # Sort clusters by similarity in descending order\n",
    "    sorted_clusters = sorted(cluster_similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    for cluster_id, avg_similarity in sorted_clusters:\n",
    "        if avg_similarity > threshold:\n",
    "            layers_to_merge = clusters[cluster_id]['layers']\n",
    "            if len(layers_to_merge) > 1:\n",
    "                print(f\"Merging Cluster {cluster_id} with layers: {layers_to_merge} and avg similarity: {avg_similarity}\")\n",
    "                model = merge_layers_fn(model, min(layers_to_merge), len(layers_to_merge)-1)\n",
    "            else:\n",
    "                print(f\"Skipping Cluster {cluster_id} with only one layer: {layers_to_merge}\")\n",
    "        else:\n",
    "            print(f\"Skipping Cluster {cluster_id}: Avg similarity {avg_similarity} below threshold.\")\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t50xtxPMH6x-",
    "outputId": "4d41fb82-423f-4c16-ba4d-d81d4b13bb04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging Cluster 4 with layers: [20, 21, 22] and avg similarity: 211.83290100097656\n",
      "Merging layer 21 into layer 20\n",
      "Merging layer 22 into layer 20\n",
      "Deleting layer 22\n",
      "Deleting layer 21\n",
      "Merging Cluster 0 with layers: [23, 24, 25, 26, 27, 28, 29] and avg similarity: 83.46148681640625\n",
      "Merging layer 24 into layer 23\n",
      "Merging layer 25 into layer 23\n",
      "Merging layer 26 into layer 23\n",
      "Merging layer 27 into layer 23\n",
      "Merging layer 28 into layer 23\n",
      "Merging layer 29 into layer 23\n",
      "Deleting layer 29\n",
      "Deleting layer 28\n",
      "Deleting layer 27\n",
      "Deleting layer 26\n",
      "Deleting layer 25\n",
      "Deleting layer 24\n",
      "Skipping Cluster 3: Avg similarity 14.73055648803711 below threshold.\n",
      "Skipping Cluster 2: Avg similarity -23.417970657348633 below threshold.\n",
      "Skipping Cluster 5: Avg similarity -141.458740234375 below threshold.\n",
      "Skipping Cluster 1: Avg similarity -539.79248046875 below threshold.\n"
     ]
    }
   ],
   "source": [
    "# Define a similarity threshold\n",
    "SIMILARITY_THRESHOLD = 80\n",
    "\n",
    "# Merge clusters\n",
    "merged_model = merge_clusters(\n",
    "    model=baichuan_model,\n",
    "    clusters=clusters,\n",
    "    cluster_similarities=similarities,\n",
    "    threshold=SIMILARITY_THRESHOLD,\n",
    "    merge_layers_fn=merge_layers_in_place_baichuan\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qCDFJTSCDo62",
    "outputId": "6b631038-8c5c-4194-9c99-168028205b92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaiChuanForCausalLM(\n",
       "  (model): Model(\n",
       "    (embed_tokens): Embedding(64000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x DecoderLayer(\n",
       "        (self_attn): Attention(\n",
       "          (W_pack): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=64000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model.config.num_hidden_layers = len(merged_model.model.layers)\n",
    "merged_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PR1TDYUlHO2r",
    "outputId": "606a6aff-c9c5-4ed1-98d5-cf869af3abe3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:638: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pruned Model Output:\n",
      "['The weather looks nice today!!..but.\\nToday we went to the beach. We went swimming too! I swim swimming swimming!\\nI got lots of beach sand too. I got sand sand s.S.s.!']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_length=150, num_return_sequences=1):\n",
    "    \"\"\"\n",
    "    Generates text from the given model and prompt.\n",
    "    Args:\n",
    "        model: HuggingFace transformer model (e.g., LLaMA).\n",
    "        tokenizer: Corresponding tokenizer for the model.\n",
    "        prompt (str): Input text prompt for generation.\n",
    "        max_length (int): Maximum length of generated text.\n",
    "        num_return_sequences (int): Number of text sequences to generate.\n",
    "    Returns:\n",
    "        List[str]: List of generated text sequences.\n",
    "    \"\"\"\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate text\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        no_repeat_ngram_size=2,  # Prevent repetitive text\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # Decode and return the generated sequences\n",
    "    return [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "\n",
    "# # Generate text with the unpruned model\n",
    "prompt = \"The weather looks nice today\"\n",
    "# unpruned_text = generate_text(llama_model, tokenizer, prompt)\n",
    "# print(\"Unpruned Model Output:\")\n",
    "# print(unpruned_text)\n",
    "\n",
    "# Generate text with the pruned model\n",
    "pruned_text = generate_text(merged_model, tokenizer, prompt)\n",
    "print(\"\\nPruned Model Output:\")\n",
    "print(pruned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VrvkPDZFUszo",
    "outputId": "0fbff7ae-2d52-4b0a-df35-1c1449f2d6ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/content/pruned_model/tokenizer_config.json',\n",
       " '/content/pruned_model/special_tokens_map.json',\n",
       " '/content/pruned_model/tokenizer.model',\n",
       " '/content/pruned_model/added_tokens.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model.save_pretrained(\"/content/pruned_model\")\n",
    "tokenizer.save_pretrained(\"/content/pruned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "g4kMPI0eU1qi"
   },
   "outputs": [],
   "source": [
    "with open(\"/content/pruned_model/README.md\", \"w\") as f:\n",
    "    f.write(\"# Merged Baichuan Model\\n\\nThis is a merged version of the Baichuan-7b model based on hyperboloid method where similarity was calculated based on hyperbolic distance. 10 layers have been merged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "id": "uWNSeBOOU4Rg",
    "outputId": "abd2bbcf-1dd4-41da-98b1-0f68b1a05d51"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/hf_api.py:9628: UserWarning: Warnings while validating metadata in README.md:\n",
      "- empty or missing yaml metadata in repo card\n",
      "  warnings.warn(f\"Warnings while validating metadata in README.md:\\n{message}\")\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/namannn/baichuan2-7b-hyperbolic-cluster-pruned/commit/59d2f672553c9307a29e696895854886e2df29b2', commit_message='Upload folder using huggingface_hub', commit_description='', oid='59d2f672553c9307a29e696895854886e2df29b2', pr_url=None, repo_url=RepoUrl('https://huggingface.co/namannn/baichuan2-7b-hyperbolic-cluster-pruned', endpoint='https://huggingface.co', repo_type='model', repo_id='namannn/baichuan2-7b-hyperbolic-cluster-pruned'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "api.create_repo(repo_id=\"namannn/baichuan2-7b-hyperbolic-cluster-pruned\", exist_ok=True)\n",
    "api.upload_folder(\n",
    "    folder_path=\"/content/pruned_model\",\n",
    "    repo_id=\"namannn/baichuan2-7b-hyperbolic-cluster-pruned\",\n",
    "    repo_type=\"model\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
